\addvspace {10\p@ }
\contentsline {figure}{\numberline {1.1}{\ignorespaces Visualization of a single unit neural network. A non-linearity is applied to the linear combination of the inputs $x_i$ with the weights $\theta _i$.}}{3}
\contentsline {figure}{\numberline {1.2}{\ignorespaces Visualization of overfitting. Training error decreases as epochs progress, eventually reaching zero. Validation error starts increasing, indicating overfitting. The best model is shown at the dotted line, where validation error reached its minimum.}}{6}
\addvspace {10\p@ }
\contentsline {figure}{\numberline {2.1}{\ignorespaces Visualization of sparse interaction (left) and dense interaction (right) between inputs and weights. With sparse interaction, the weights only interact on a subregion of the entire input. This is in contrast to the dense interaction of classic neural networks, where there's an interaction between every input and every output.}}{9}
\contentsline {figure}{\numberline {2.2}{\ignorespaces Visualization of embeddings using the t-distributed stochastic neighbor embedding (t-SNE) dimensionality reduction algorithm. Words with similar or related meanings tend to lie close to each other in embedding space.}}{11}
\contentsline {figure}{\numberline {2.3}{\ignorespaces Visualization of a feature map with a single kernel. In this example, the kernel convolves tri-grams, or windows of three consecutive words. After convolution with all possible trigram context windows, max pooling is applied to reduce dimensionality. Here, the pool size is 3. This process is repeated as many times as there are kernels in the layer and their outputs are concatenated horizantally to yield a matrix of size \textit {num filters} $\times $ (\textit {input length - pool size + 1}).}}{13}
\addvspace {10\p@ }
\contentsline {figure}{\numberline {3.1}{\ignorespaces A common architecture for a convolutional network for text classification is an embedding layer followed by a feature map, then a dense layer to compute class probabilities.}}{14}
\contentsline {figure}{\numberline {3.2}{\ignorespaces Visualization of an embedding layer. Word indexes are mapped to word embeddings via table lookups. This layer outputs a matrix comprised of stacked embeddings, one for each index.}}{16}
\contentsline {figure}{\numberline {3.3}{\ignorespaces Visualization of a recurrent layer. Each hidden layer $\bm {h}^{\textit {t}}$ is a function of the previous hidden layer $\bm {h}^{\textit {t-1}}$ and the present input signal $\bm {x}^{\textit {t}}$. The weights are shared across time steps in order for the model to generalize better.}}{17}
\contentsline {figure}{\numberline {3.4}{\ignorespaces Model Architecture: embedding layer, followed by a feature map, and a recurrent layer. At the end, we have a fully connected layer with a softmax activation, which will output class probabilities.}}{19}
\addvspace {10\p@ }
\addvspace {10\p@ }
\addvspace {10\p@ }
