\relax 
\citation{le2013building}
\citation{lecun1989backpropagation}
\citation{oquab2014learning}
\citation{dosovitskiy2014discriminative}
\citation{hochreiter1997long}
\citation{chung2014empirical}
\@writefile{toc}{\contentsline {chapter}{\numberline {2}Text Classification with Deep Neural Networks}{6}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{TXT-CLASS}{{2}{6}}
\@writefile{toc}{\contentsline {section}{\numberline {2.1}Brief Overview of Deep Learning}{6}}
\citation{bengio2003neural}
\citation{zhou1988image}
\@writefile{toc}{\contentsline {section}{\numberline {2.2}Word Embeddings}{7}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.1}{\ignorespaces Visualization of embeddings using the t-distributed stochastic neighbor embedding (t-SNE) dimensionality reduction algorithm. Words with similar or related meanings tend to lie close to each other in embedding space.}}{8}}
\@writefile{toc}{\contentsline {section}{\numberline {2.3}Convolutional Neural Networks}{8}}
\citation{kim2014convolutional}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3.1}Input Representation: Word Embeddings for Convolutional Networks}{9}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.2}{\ignorespaces Visualization of a feature map with a single kernel. In this example, the kernel convolves tri-grams, or windows of three words. After convolution with all possible trigram context windows, max pooling is applied to reduce dimensionality. Here, the pool size is 3. This process is repeated as many times as there are kernels in the layer and their outputs are concatenated horizantally to yield a matrix of size \textit  {num filters} $\times $ (\textit  {input length - pool size + 1}).}}{10}}
\newlabel{fig:convolution}{{2.2}{10}}
\@setckpt{chapter2}{
\setcounter{page}{11}
\setcounter{equation}{0}
\setcounter{enumi}{0}
\setcounter{enumii}{0}
\setcounter{enumiii}{0}
\setcounter{enumiv}{0}
\setcounter{footnote}{0}
\setcounter{mpfootnote}{0}
\setcounter{part}{0}
\setcounter{chapter}{2}
\setcounter{section}{3}
\setcounter{subsection}{1}
\setcounter{subsubsection}{0}
\setcounter{paragraph}{0}
\setcounter{subparagraph}{0}
\setcounter{figure}{2}
\setcounter{table}{0}
\setcounter{definition}{0}
\setcounter{theorem}{0}
\setcounter{corollary}{0}
\setcounter{parentequation}{0}
\setcounter{ALG@line}{0}
\setcounter{ALG@rem}{0}
\setcounter{ALG@nested}{0}
\setcounter{ALG@Lnr}{2}
\setcounter{ALG@blocknr}{10}
\setcounter{ALG@storecount}{0}
\setcounter{ALG@tmpcounter}{0}
\setcounter{float@type}{8}
\setcounter{algorithm}{0}
}
