\relax 
\citation{le2013building}
\citation{dosovitskiy2014discriminative}
\citation{lecun1989backpropagation}
\citation{oquab2014learning}
\@writefile{toc}{\contentsline {chapter}{\numberline {2}Text Classification with Deep Neural Networks}{8}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{TXT-CLASS}{{2}{8}}
\@writefile{toc}{\contentsline {section}{\numberline {2.1}Brief Overview of Deep Learning}{8}}
\citation{siegelmann1995computational}
\citation{hochreiter1997long}
\citation{chung2014empirical}
\@writefile{lof}{\contentsline {figure}{\numberline {2.1}{\ignorespaces Visualization of sparse interaction (left) and dense interaction (right) between inputs and weights. With sparse interaction, the weights only interact on a subregion of the entire input. This is in contrast to the dense interaction of classic neural networks, where there's an interaction between every input and every output.}}{9}}
\citation{bengio2003neural}
\citation{zhou1988image}
\@writefile{toc}{\contentsline {section}{\numberline {2.2}Word Embeddings}{10}}
\@writefile{toc}{\contentsline {section}{\numberline {2.3}Convolutional Neural Networks}{10}}
\citation{kim2014convolutional}
\@writefile{lof}{\contentsline {figure}{\numberline {2.2}{\ignorespaces Visualization of embeddings using the t-distributed stochastic neighbor embedding (t-SNE) dimensionality reduction algorithm. Words with similar or related meanings tend to lie close to each other in embedding space.}}{11}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3.1}Input Representation: Word Embeddings for Convolutional Networks}{12}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.3}{\ignorespaces Visualization of a feature map with a single kernel. In this example, the kernel convolves tri-grams, or windows of three consecutive words. After convolution with all possible trigram context windows, max pooling is applied to reduce dimensionality. Here, the pool size is 3. This process is repeated as many times as there are kernels in the layer and their outputs are concatenated horizantally to yield a matrix of size \textit  {num filters} $\times $ (\textit  {input length - pool size + 1}).}}{13}}
\newlabel{fig:convolution}{{2.3}{13}}
\@setckpt{chapter2}{
\setcounter{page}{14}
\setcounter{equation}{0}
\setcounter{enumi}{0}
\setcounter{enumii}{0}
\setcounter{enumiii}{0}
\setcounter{enumiv}{0}
\setcounter{footnote}{0}
\setcounter{mpfootnote}{0}
\setcounter{part}{0}
\setcounter{chapter}{2}
\setcounter{section}{3}
\setcounter{subsection}{1}
\setcounter{subsubsection}{0}
\setcounter{paragraph}{0}
\setcounter{subparagraph}{0}
\setcounter{figure}{3}
\setcounter{table}{0}
\setcounter{definition}{0}
\setcounter{theorem}{0}
\setcounter{corollary}{0}
\setcounter{parentequation}{0}
\setcounter{ALG@line}{0}
\setcounter{ALG@rem}{0}
\setcounter{ALG@nested}{0}
\setcounter{ALG@Lnr}{2}
\setcounter{ALG@blocknr}{10}
\setcounter{ALG@storecount}{0}
\setcounter{ALG@tmpcounter}{0}
\setcounter{float@type}{8}
\setcounter{algorithm}{0}
}
