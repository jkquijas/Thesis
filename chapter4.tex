% chap3.tex (Definitions and Theorem)

\chapter{Towards Improving Model Performance: Embeddings and Data Augmentation}
As mentioned in [REFER TO SECTION], we can fine-tune the word embeddings via back-propagation and gradient descent (or some other
gradient-based optimizer), just as any other weights in the neural network. In this case, the number of parameters in our model increases drastically.
With more free parameters to learn from the data, it is easier to overfit. This is especially true if the amount of training data is limited, as is the case
with our particular task. Another important aspect to consider when fine-tuning the embeddings is whether we lose the semantic information originally
present in them. It is therefore a good idea to quantify the effect of training the embeddings. In the following sections, we propose different ways
treat the embedding layer in our neural network in order to mitigate overfitting. We also consider reducing the dimensionality of the embeddings
via Principal Component Analysis (PCA). Finally, we propose data augmentation techniques inspired from computer vision tasks but designed for
text data with temporal structure, as is the case with our data.

\section{Freezing the Embedding Layer}
When we make our model's embeddings trainable, its number of parameters highly increases. With inputs of size $n$,
and embeddings of size $d$, we introduce $n \times d$ new parameters into the model.
We therefore propose multiple embedding \textit{trainability} schemes to quantify how much the embeddings help increase (or decrease) test
accuracy and overfitting.

\begin{itemize}
  \item{Frozen Embeddings}
  \begin{itemize}
    \item{With this scheme, we \textit{freeze} the embedding layer altogether. Not once during training do we update
    the embedding vectors, so the model relies completely on its other parameters to fit the data.}
  \end{itemize}
  \item{Freeze-Unfreeze}
  \begin{itemize}
    \item{With the \textit{freeze-unfreeze} scheme, we train the model with its embedding layer frozen until some
    stopping criterion is met e.g. early stopping due to validation error increase. After this, we \textit{unfreeze}, or make the
    embedding layer trainable, and retrain the model.}
  \end{itemize}
  \item{Flash-Freeze}
  \begin{itemize}
    \item{In the \textit{flash-freeze} scheme, we allow the embedding layer to be trainable for a small number of
    epochs e.g. two, then freeze it and retrain the model.}
  \end{itemize}
  \item{Unfrozen Embeddings}
  \begin{itemize}
    \item{We allow the embeddings layer to be trainable from the very beggining. We consider this training scheme as our \textbf{baseline}.}
  \end{itemize}
\end{itemize}

\section{Principal Component Analysis and Dimensionality Reduction of the Embeddings}
Principal Component Analysis (PCA) is a very popular linear transformation commonly used to reduce dimensionality of a dataset CITE[Shlens 2005].
It projects the data to a representation where each variable is linearly uncorrelated. Concretely, PCA rotates the data onto new axes called principal
components. Given a data matrix $\bm{X}$ with $n$ dimensions, its principal components are the eigenvectors of the covariance matrix $\bm{X}^T\bm{X}$. The first principal component is the direction of most variation. The second principal component is the direction with the next largest amount
of variation, and so on. We can then choose to retain only the $k<n$ principal components that contain the most amount of variation in the data.

We propose to apply PCA to the word embeddings in order to linearly decorrelate their dimensions and reduce their dimensionality.
By reducing the dimensionality while preserving the majority of the variance in them (e.g. 95\%), we will reduce the amount of parameters introduced
to the model while retaining most of the information from the original embeddings.



\section{Dataset Augmentation: Shuffling and Noise Injection}
Another common practice aimed to reducing overfitting is to \textbf{augment} the dataset.
Data augmentation refers to any transformation of the input data in a way that
the label value does not change. Data augmentation is ubiquitous in computer vision tasks. Example transformations include translations,
rotations, and color intensity jitters via Pricipal Component Analysis CITE[krizhevsky 2012]. All these tranformations should be subtle enough that the overall structure is preserved, but
allow for the model to process more distinct training data points.

The data augmentation techniques mentioned above are common practive in computer vision tasks such as image classification.
In order to introduce more variance into our dataset, we propose data augmentation techniques inspired by computer vision by designed for
text sequence data.
We propose to \textbf{shuffle} by randomly changing the order of words within a \textbf{context windows}, or non-overlapping neighborhood of words
in the input text sequence.
We further propose to inject small amounts of \textbf{noise} to the input sequence by randomly replacing words with words taken from the training vocabulary.

As long as the overall structure of the data (and label) is preserved, data augmentation normally lead to improved task performance CITE[Chawla et al 2002]
CITE[Krizhevsky et al 2012]CITE[Simonyan and Zisserman 2015] CITE[He et al 2016].
With that same rationale, we expect our proposed augmentation techniques to yield increase accuracy percentages. In other words, we expect that
our changes will be subtle enough for the label to be preserved, but effective enough that we introduce more variance into our dataset and help cope with
the limited amount of training examples.


\subsection{Shuffling}
We propose to augment our dataset by making small \textit{context} changes that don't change the global structure
of the input sequences. Concretely, we move a \textit{context window} along non-overlapping neighborhoods the input sequence, randomly
shuffling the words indexes inside it. For example using a context window of size 2 i.e. \textit{bi-gram}, an input sequence
\[\bm{x} = x_1, x_2, x_3, x_4, x_5, x_6\]

 could be shuffled to:
\[\bm{x}^{\prime} = x_2, x_1, x_3, x_4, \underbrace{x_6, x_5}_\text{bi-gram}\]
In the first pass, the first two indexes get shuffled. The second pass led to no changes in ordering. The third and
final pass shuffles the last two indexes.

As it is common practice in computer vision tasks to apply small translations and rotations to images, we perform
this operation to slightly perturb the temporal structure of our text sequence data.
The rationale for this dataset augmentation technique is that small changes in the ordering of the words will
result in a larger training sample; a training input sequence has a low chance of being repeated as its
length increases. Smaller context windows preserve the most structure. This method will preserve
most of the original context structure in the sequence, as long as the shuffling is not to harsh e.g.
a complete random permutation of the sequence.

\subsection{Noise Injection}
One common augmentation technique for image datasets is to add small amount of noise. By replacing a relatively small
amount of pixel values with noise, the model gets to process and train on a different but similar instance. The noise should be subtle
enough as to not distort the image too much, otherwise we could potentially train the model using mostly noise.
We propose to augment our dataset by injecting small amounts of noise to each training sample. Again, we aim to simulate
a larger training sample while avoiding harsh changes to the original inputs. Concretely, each word in a text sequence gets replaced
with a specified probability with a word randomly chosen from our training vocabulary.
\begin{algorithm}[H]
\caption{Add noise to input sequence}
\begin{algorithmic}[1]
\Procedure{NoiseInjection}{$\bm{x}=[x_1,...,x_n], \mathbb{V}, p_{noise}$}
\For {$k$ = 1 to $n$}
\If{$p_k \sim \textit{U}(0,1) \leq p_{noise}$}
\State$x_k \gets x^{\prime}_k \in \mathbb{V}$
\EndIf
\EndFor

\Return $\bm{x}$
\EndProcedure
\end{algorithmic}
\end{algorithm}


\section{Dataset Augmentation: Padding}

Although more complex neural models are designed to cope with variable length input,
in practice a more common and simple approach is to pad data to be of some specified
length as described in chapter [CITE CHAPTER].
In order to enforce uniform input size for our neural networks,
we apply \textbf{zero-padding}.For any arbitrary training instance $BoW(\bm{s})=$ $\bm{x} = x_1,...,x_k$, we enforce that $k = n$, for the specified input size
$n$. Thus, if $k \textless n$, we transform it into $\bm{x}_{pad} = x_1,...,x_k, 0_{k+1}, ..., 0_{n}$. Conversely, if
$k \textgreater n$, we simply truncate $\bm{x}$ to be of size $n$. The input length introduces another model hyper-parameter
that should be fine-tuned, but a reasonable approach is to pad enough to fully accomodate
the length of most input sequences i.e. it’s preferrable to pad than truncate and lose
information.

Our network's input is a sequence of integers, each integer being a word index: a number representing a word in our vocabulary $\mathbb{V}$.
Word indexes range from 1 to $|\mathbb{V}|$, and the index 0 being left for out-of-vocabulary words.
When we pad our input sequence with 0’s, we don’t add any additional information; we
simply create a constant input length. We propose that if instead we add values that characterize or help
describe the input sequence more thoroughly, we may increase the amount of useful information
available during neural network training.

Consider a very simple input sequence, and assume its true label can be determined
from a single word:

\[\text{"This paper is about computer graphics"}\]

where the label is "Graphics" i.e. a publication about computer graphics.
In this simple case, the label is determined by the word "graphics". To the human reader, this single informative word
is enough for the classifier to predict the label correctly although it is only 1/6 of the entire text.

Now consider a the padded version, where we enfore an input length of 10:

\[\text{"This paper is about computer graphics PAD PAD PAD PAD"}\]

In the padded version, the word graphics is now only 1/10 of the entire text. If we
could pad the sequence using this informative word instead of some
meaningless token e.g. 0's, we could increase the likelihood of the model extracting features from it. In other
words, make important words be present more frequently.
We propose to pad input sequences with values found already within the
input text sequence instead of 0's only.

\subsection{Wrap Padding}
Our proposed padding scheme is to \textit{wrap around} the text, repeating words once we reach the padding portion.
Refering back to the first example, the input sequence:
\[\text{"This paper is about computer graphics PAD PAD PAD PAD"}\]

would then be
\[\text{"This paper is about computer graphics This paper is about"}\]

One thing to observe is that using this simple scheme, we remove all 0's i.e. non-informative padding indexes
from the text sequence, but we also don't have selection mechanism and can miss useful words such as missing the
word "\textit{graphics}" in this example. Nonetheless, it is a simple approach to pad our data and increase the likelihood of encountering informative
words during feature extraction.


\section{Reducing Data Granularity: From Abstract to Sentences}
Our last proposed dataset augmentation scheme is break each training abtract into a set of sentences, and train
the network using this finer text granularity. For example, training a model using single sentences, another using sentence pairs,
and another using sentence triplets.
During testing, we break a test abstract into sentence sets and classify each set individually. We then assign
the class with the largest mean.
