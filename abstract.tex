% abstract.tex (Abstract)

\addcontentsline{toc}{chapter}{Abstract}

\chapter*{Abstract}
Convolutional neural networks have seen much success in computer vision and natural language processing tasks.
When training convolutional neural networks for text classification tasks, a common technique is to transform an input
sequence of words into a dense matrix of word embeddings, or words represented as dense vectors, using table lookup
operations. This allows for the inputs to be represented in a way that the well-known convolution/pooling operations can be applied
to them in a manner similar to images. These word embeddings may be further incorporated into the neural
network itself as a trainable layer to allow fine-tuning, usually leading to improved model predictions.
The drastic increase of free parameters however, leads to overfitting if proper regularization is not applied or the size of
the training set is not large enough.

We give an overview of popular convolutional and recurrent network architectures, describe their basic functions, and discuss their
observed advantages and shortcomings in our experiments. We follow this discussion with an overview of our final choice of architecture,
based on a combination of these architectures.

We train our neural networks using abstracts from multiple science and engineering fields; each set of
abstracts comprised of multiple topics. The number of publications available for our task is moderate, in the mid thousands for each topic.
We analyse the effect of using word embeddings with our models in terms of fit and prediction. We then propose embedding "trainability" schemes
to alleviate overfitting, improve test accuracy and reduce training times. We conclude our study proposing several data augmentation
techniques designed for text sequences in an attempt to further mitigate overfitting and improve generalization.
Finally, we provide discussion on our empirical results and propose promising directions for future work.
