% chap2.tex (Definitions)

\chapter{Text Classification with Deep Neural Networks}\label{TXT-CLASS}

\section{Word Embeddings}
A very common and simple vector respresentation for words is the one-hot representation. The length
of a one-hot vector is the size of the data vocabulary i.e. how many distinct words are found in our data set. For any
word, its one-hot vector is zeros everywhere except for a 1 at the word's index. This representation, although simple,
fails to capture any meaning other than an identifier.
Neural language models i.e. a language model learned using a network, learn to represent words as continuous, dense vectors.
These dense, continouos vector representations are commonly called word embeddings. Due to the of the underlying algorithm
used to learn these word embeddings, similar words tend to lie closer to each other on embedding space.
Because of this, word embeddings are said to capture semantic relations, and thus encode more information than just a word identifier.

[SHOW EMBEDDING PROJECTION]


\section{Convolutional Neural Networks}
Convolutional neural networks are known for their abilities to learn high-level features from raw data. As input signals advance
forward through the network, they produce latent signals as linear conbinations with learned parameters, have non-linearities applied
to them, and have a pooling or selection mechanism based on simple functions such as the average or maximum operations.

When dealing with image data, images are convolved with multiple filters, each convolution applied to overlapping subimages called as receptive fields.
This localized convolution process leads to discovery of low level features of images in the training set such as edges. As data
flows forward through the model, higher level features are discovered e.g. wheels or headlights in a vehicle image dataset.

These networks are comprised of \textit{feature maps}. A feature map is a \textbf{convolution} layer paired with a
\textbf{pooling} layer afterwards. The convolution stage creates \textit{activations}, whereas the pooling stage
reduces dimensionality and creates translation invariance[CITE].

[INCLUDE IMAGE]

We can take advantage of word embeddings to apply convolutions to text in a fashion similar to convolutions with
image data. We apply the convolutions to overlapping sub-regions of the input text i.e. bi-grams, tri-grams, etc.
After convolving these individual and overlapping subregions, we apply a non-linear function and reduce data dimensionality
by pooling.

[DEVELOP]

\section{Input Representation: Integer Sequences to Word Embeddings}
Given a set of texts $\bm{s_1},...,\bm{s_m}$, we build a vocabulary $\mathbb{V}$ and a bag-of-words model from $\mathbb{V}$ to create a mapping $BoW:\mathbb{V} \mapsto \{1,...,|\mathbb{V}|\}$.
We represent a training text $\bm{s_i}$ as a sequence of integers $BoW(\bm{s})=$ $\bm{x} = x_1,...,x_k$, each integer being simply a word index
in $\mathbb{V}$.

%%% Local Variables:
%%% mode: latex
%%% TeX-master: "thesis"
%%% End:
