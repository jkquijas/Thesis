\relax 
\citation{kim2014convolutional}
\@writefile{toc}{\contentsline {chapter}{\numberline {3}Model, Dataset, and Final Pipeline Description}{14}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {3.1}Model Description}{14}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.1}{\ignorespaces A common architecture for a convolutional network for text classification is an embedding layer followed by a feature map, then a dense layer to compute class probabilities.}}{14}}
\citation{yin2017comparative}
\@writefile{toc}{\contentsline {section}{\numberline {3.2}Layer Descriptions}{15}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2.1}Embedding Layer}{15}}
\newlabel{embeddinglayer}{{3.2.1}{15}}
\citation{waibel1988phoneme}
\@writefile{lof}{\contentsline {figure}{\numberline {3.2}{\ignorespaces Visualization of an embedding layer. Word indexes are mapped to word embeddings via table lookups. This layer outputs a matrix comprised of stacked embeddings, one for each index.}}{16}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2.2}Feature Maps: Convolution + Pooling}{16}}
\citation{DBLP:journals/corr/LinCY13}
\citation{rumelhart1986sequential}
\citation{chung2014empirical}
\citation{hochreiter1997long}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2.3}Gated Recurrent Unit Layer}{17}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.3}{\ignorespaces Visualization of a recurrent layer. Each hidden layer $\bm  {h}^{\textit  {t}}$ is a function of the previous hidden layer $\bm  {h}^{\textit  {t-1}}$ and the present input signal $\bm  {x}^{\textit  {t}}$. The weights are shared across time steps in order for the model to generalize better.}}{17}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2.4}Dense Layer}{18}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.4}{\ignorespaces Model Architecture: embedding layer, followed by a feature map, and a recurrent layer. At the end, we have a fully connected layer with a softmax activation, which will output class probabilities.}}{19}}
\@setckpt{chapter3}{
\setcounter{page}{21}
\setcounter{equation}{0}
\setcounter{enumi}{0}
\setcounter{enumii}{0}
\setcounter{enumiii}{0}
\setcounter{enumiv}{0}
\setcounter{footnote}{0}
\setcounter{mpfootnote}{0}
\setcounter{part}{0}
\setcounter{chapter}{3}
\setcounter{section}{2}
\setcounter{subsection}{4}
\setcounter{subsubsection}{0}
\setcounter{paragraph}{0}
\setcounter{subparagraph}{0}
\setcounter{figure}{4}
\setcounter{table}{0}
\setcounter{definition}{0}
\setcounter{theorem}{0}
\setcounter{corollary}{0}
\setcounter{parentequation}{0}
\setcounter{ALG@line}{0}
\setcounter{ALG@rem}{0}
\setcounter{ALG@nested}{0}
\setcounter{ALG@Lnr}{2}
\setcounter{ALG@blocknr}{10}
\setcounter{ALG@storecount}{0}
\setcounter{ALG@tmpcounter}{0}
\setcounter{float@type}{8}
\setcounter{algorithm}{0}
}
