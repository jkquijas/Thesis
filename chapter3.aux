\relax 
\@writefile{toc}{\contentsline {chapter}{\numberline {3}Model, Dataset, and Final Pipeline Description}{9}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {3.1}Layer Descriptions}{9}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1.1}Embedding Layer}{9}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.1}{\ignorespaces Visualization of an embedding layer. Word indexes are mapped to word embeddings. This layer outputs a matrix comprised of stacked embeddings, one for each index.}}{10}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1.2}Feature Maps: Convolution + Pooling}{10}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1.3}Gated Recurrent Unit Layer}{11}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.2}{\ignorespaces Visualization of a recurrent layer. Each hidden layer $\bm  {h}^{\textit  {t}}$ is a function of the previous hidden layer $\bm  {h}^{\textit  {t-1}}$, the present input signal $\bm  {x}^{\textit  {t}}$. The weights are shared across time steps in order for the model to generalize better.}}{11}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1.4}Dense Layer}{12}}
\@writefile{toc}{\contentsline {section}{\numberline {3.2}Model Description}{12}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.3}{\ignorespaces Model Architecture: embedding layer, followed by a feature map, and a recurrent layer. At the end, we have a fully connected layer with a softmax activation, which will output class probabilities.}}{14}}
\@setckpt{chapter3}{
\setcounter{page}{15}
\setcounter{equation}{0}
\setcounter{enumi}{0}
\setcounter{enumii}{0}
\setcounter{enumiii}{0}
\setcounter{enumiv}{0}
\setcounter{footnote}{0}
\setcounter{mpfootnote}{0}
\setcounter{part}{0}
\setcounter{chapter}{3}
\setcounter{section}{2}
\setcounter{subsection}{0}
\setcounter{subsubsection}{0}
\setcounter{paragraph}{0}
\setcounter{subparagraph}{0}
\setcounter{figure}{3}
\setcounter{table}{0}
\setcounter{definition}{0}
\setcounter{theorem}{0}
\setcounter{corollary}{0}
\setcounter{parentequation}{0}
\setcounter{ALG@line}{0}
\setcounter{ALG@rem}{0}
\setcounter{ALG@nested}{0}
\setcounter{ALG@Lnr}{2}
\setcounter{ALG@blocknr}{10}
\setcounter{ALG@storecount}{0}
\setcounter{ALG@tmpcounter}{0}
\setcounter{float@type}{8}
\setcounter{algorithm}{0}
}
